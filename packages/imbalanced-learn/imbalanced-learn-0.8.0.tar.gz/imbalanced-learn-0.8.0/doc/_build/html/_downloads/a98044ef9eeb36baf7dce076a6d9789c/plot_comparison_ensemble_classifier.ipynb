{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Compare ensemble classifiers using resampling\n\n\nEnsembling classifiers have shown to improve classification performance compare\nto single learner. However, they will be affected by class imbalance. This\nexample shows the benefit of balancing the training set before to learn\nlearners. We are making the comparison with non-balanced ensemble methods.\n\nWe make a comparison using the balanced accuracy and geometric mean which are\nmetrics widely used in the literature to evaluate models learned on imbalanced\nset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n# License: MIT\n\nimport itertools\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import balanced_accuracy_score\n\nfrom imblearn.datasets import fetch_datasets\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom imblearn.ensemble import EasyEnsembleClassifier\nfrom imblearn.ensemble import RUSBoostClassifier\n\nfrom imblearn.metrics import geometric_mean_score\n\n\ndef plot_confusion_matrix(\n    cm,\n    classes,\n    ax,\n    normalize=False,\n    title=\"Confusion matrix\",\n    cmap=plt.cm.Blues,\n):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    print(cm)\n    print(\"\")\n\n    ax.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n    ax.set_title(title)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.sca(ax)\n    plt.yticks(tick_marks, classes)\n\n    fmt = \".2f\" if normalize else \"d\"\n    thresh = cm.max() / 2.0\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        ax.text(\n            j,\n            i,\n            format(cm[i, j], fmt),\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\",\n        )\n\n    ax.set_ylabel(\"True label\")\n    ax.set_xlabel(\"Predicted label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load an imbalanced dataset\n##############################################################################\n We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1\n (number of majority sample for a minority sample). The data are then split\n into training and testing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "satimage = fetch_datasets()[\"satimage\"]\nX, y = satimage.data, satimage.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification using a single decision tree\n##############################################################################\n We train a decision tree classifier which will be used as a baseline for the\n rest of this example.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results are reported in terms of balanced accuracy and geometric mean\nwhich are metrics widely used in the literature to validate model trained on\nimbalanced set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tree = DecisionTreeClassifier()\ntree.fit(X_train, y_train)\ny_pred_tree = tree.predict(X_test)\nprint(\"Decision tree classifier performance:\")\nprint(\n    f\"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_tree):.2f} - \"\n    f\"Geometric mean {geometric_mean_score(y_test, y_pred_tree):.2f}\"\n)\ncm_tree = confusion_matrix(y_test, y_pred_tree)\nfig, ax = plt.subplots()\nplot_confusion_matrix(\n    cm_tree, classes=np.unique(satimage.target), ax=ax, title=\"Decision tree\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification using bagging classifier with and without sampling\n##############################################################################\n Instead of using a single tree, we will check if an ensemble of decsion tree\n can actually alleviate the issue induced by the class imbalancing. First, we\n will use a bagging classifier and its counter part which internally uses a\n random under-sampling to balanced each boostrap sample.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bagging = BaggingClassifier(n_estimators=50, random_state=0)\nbalanced_bagging = BalancedBaggingClassifier(n_estimators=50, random_state=0)\n\nbagging.fit(X_train, y_train)\nbalanced_bagging.fit(X_train, y_train)\n\ny_pred_bc = bagging.predict(X_test)\ny_pred_bbc = balanced_bagging.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Balancing each bootstrap sample allows to increase significantly the balanced\naccuracy and the geometric mean.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Bagging classifier performance:\")\nprint(\n    f\"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_bc):.2f} - \"\n    f\"Geometric mean {geometric_mean_score(y_test, y_pred_bc):.2f}\"\n)\ncm_bagging = confusion_matrix(y_test, y_pred_bc)\nfig, ax = plt.subplots(ncols=2)\nplot_confusion_matrix(\n    cm_bagging, classes=np.unique(satimage.target), ax=ax[0], title=\"Bagging\"\n)\n\nprint(\"Balanced Bagging classifier performance:\")\nprint(\n    f\"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_bbc):.2f} - \"\n    f\"Geometric mean {geometric_mean_score(y_test, y_pred_bbc):.2f}\"\n)\ncm_balanced_bagging = confusion_matrix(y_test, y_pred_bbc)\nplot_confusion_matrix(\n    cm_balanced_bagging,\n    classes=np.unique(satimage.target),\n    ax=ax[1],\n    title=\"Balanced bagging\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification using random forest classifier with and without sampling\n##############################################################################\n Random forest is another popular ensemble method and it is usually\n outperforming bagging. Here, we used a vanilla random forest and its balanced\n counterpart in which each bootstrap sample is balanced.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(n_estimators=50, random_state=0)\nbrf = BalancedRandomForestClassifier(n_estimators=50, random_state=0)\n\nrf.fit(X_train, y_train)\nbrf.fit(X_train, y_train)\n\ny_pred_rf = rf.predict(X_test)\ny_pred_brf = brf.predict(X_test)\n\n# Similarly to the previous experiment, the balanced classifier outperform the\n# classifier which learn from imbalanced bootstrap samples. In addition, random\n# forest outsperforms the bagging classifier.\n\nprint(\"Random Forest classifier performance:\")\nprint(\n    f\"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_rf):.2f} - \"\n    f\"Geometric mean {geometric_mean_score(y_test, y_pred_rf):.2f}\"\n)\ncm_rf = confusion_matrix(y_test, y_pred_rf)\nfig, ax = plt.subplots(ncols=2)\nplot_confusion_matrix(\n    cm_rf, classes=np.unique(satimage.target), ax=ax[0], title=\"Random forest\"\n)\n\nprint(\"Balanced Random Forest classifier performance:\")\nprint(\n    f\"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_brf):.2f} - \"\n    f\"Geometric mean {geometric_mean_score(y_test, y_pred_brf):.2f}\"\n)\ncm_brf = confusion_matrix(y_test, y_pred_brf)\nplot_confusion_matrix(\n    cm_brf,\n    classes=np.unique(satimage.target),\n    ax=ax[1],\n    title=\"Balanced random forest\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Boosting classifier\n##############################################################################\n In the same manner, easy ensemble classifier is a bag of balanced AdaBoost\n classifier. However, it will be slower to train than random forest and will\n achieve worse performance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "base_estimator = AdaBoostClassifier(n_estimators=10)\neec = EasyEnsembleClassifier(n_estimators=10, base_estimator=base_estimator)\neec.fit(X_train, y_train)\ny_pred_eec = eec.predict(X_test)\nprint(\"Easy ensemble classifier performance:\")\nprint(\n    f\"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_eec):.2f} - \"\n    f\"Geometric mean {geometric_mean_score(y_test, y_pred_eec):.2f}\"\n)\ncm_eec = confusion_matrix(y_test, y_pred_eec)\nfig, ax = plt.subplots(ncols=2)\nplot_confusion_matrix(\n    cm_eec,\n    classes=np.unique(satimage.target),\n    ax=ax[0],\n    title=\"Easy ensemble classifier\",\n)\n\nrusboost = RUSBoostClassifier(n_estimators=10, base_estimator=base_estimator)\nrusboost.fit(X_train, y_train)\ny_pred_rusboost = rusboost.predict(X_test)\nprint(\"RUSBoost classifier performance:\")\nprint(\n    f\"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_rusboost):.2f} - \"\n    f\"Geometric mean {geometric_mean_score(y_test, y_pred_rusboost):.2f}\"\n)\ncm_rusboost = confusion_matrix(y_test, y_pred_rusboost)\nplot_confusion_matrix(\n    cm_rusboost,\n    classes=np.unique(satimage.target),\n    ax=ax[1],\n    title=\"RUSBoost classifier\",\n)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}