{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Compare under-sampling samplers\n\n\nThe following example attends to make a qualitative comparison between the\ndifferent under-sampling algorithms available in the imbalanced-learn package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n# License: MIT\n\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\n\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import (\n    ClusterCentroids,\n    RandomUnderSampler,\n    NearMiss,\n    InstanceHardnessThreshold,\n    CondensedNearestNeighbour,\n    EditedNearestNeighbours,\n    RepeatedEditedNearestNeighbours,\n    AllKNN,\n    NeighbourhoodCleaningRule,\n    OneSidedSelection,\n)\n\nprint(__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function will be used to create toy dataset. It using the\n``make_classification`` from scikit-learn but fixing some parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_dataset(\n    n_samples=1000,\n    weights=(0.01, 0.01, 0.98),\n    n_classes=3,\n    class_sep=0.8,\n    n_clusters=1,\n):\n    return make_classification(\n        n_samples=n_samples,\n        n_features=2,\n        n_informative=2,\n        n_redundant=0,\n        n_repeated=0,\n        n_classes=n_classes,\n        n_clusters_per_class=n_clusters,\n        weights=list(weights),\n        class_sep=class_sep,\n        random_state=0,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function will be used to plot the sample space after resampling\nto illustrate the characteristic of an algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_resampling(X, y, sampling, ax):\n    X_res, y_res = sampling.fit_resample(X, y)\n    ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor=\"k\")\n    # make nice plotting\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines[\"left\"].set_position((\"outward\", 10))\n    ax.spines[\"bottom\"].set_position((\"outward\", 10))\n    return Counter(y_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function will be used to plot the decision function of a\nclassifier given some data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_decision_function(X, y, clf, ax):\n    plot_step = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(\n        np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)\n    )\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    ax.contourf(xx, yy, Z, alpha=0.4)\n    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor=\"k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prototype generation: under-sampling by generating new samples\n##############################################################################\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``ClusterCentroids`` under-samples by replacing the original samples by the\ncentroids of the cluster found.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94), class_sep=0.8)\n\nclf = LinearSVC().fit(X, y)\nplot_decision_function(X, y, clf, ax1)\nax1.set_title(f\"Linear SVC with y={Counter(y)}\")\nsampler = ClusterCentroids(random_state=0)\nclf = make_pipeline(sampler, LinearSVC())\nclf.fit(X, y)\nplot_decision_function(X, y, clf, ax2)\nax2.set_title(f\"Decision function for {sampler.__class__.__name__}\")\nplot_resampling(X, y, sampler, ax3)\nax3.set_title(f\"Resampling using {sampler.__class__.__name__}\")\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prototype selection: under-sampling by selecting existing samples\n##############################################################################\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The algorithm performing prototype selection can be subdivided into two\ngroups: (i) the controlled under-sampling methods and (ii) the cleaning\nunder-sampling methods.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the controlled under-sampling methods, the number of samples to be\nselected can be specified. ``RandomUnderSampler`` is the most naive way of\nperforming such selection by randomly selecting a given number of samples by\nthe targetted class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94), class_sep=0.8)\n\nclf = LinearSVC().fit(X, y)\nplot_decision_function(X, y, clf, ax1)\nax1.set_title(f\"Linear SVC with y={Counter(y)}\")\nsampler = RandomUnderSampler(random_state=0)\nclf = make_pipeline(sampler, LinearSVC())\nclf.fit(X, y)\nplot_decision_function(X, y, clf, ax2)\nax2.set_title(f\"Decision function for {sampler.__class__.__name__}\")\nplot_resampling(X, y, sampler, ax3)\nax3.set_title(f\"Resampling using {sampler.__class__.__name__}\")\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``NearMiss`` algorithms implement some heuristic rules in order to select\nsamples. NearMiss-1 selects samples from the majority class for which the\naverage distance of the $k$` nearest samples of the minority class is\nthe smallest. NearMiss-2 selects the samples from the majority class for\nwhich the average distance to the farthest samples of the negative class is\nthe smallest. NearMiss-3 is a 2-step algorithm: first, for each minority\nsample, their :$m$ nearest-neighbors will be kept; then, the majority\nsamples selected are the on for which the average distance to the $k$\nnearest neighbors is the largest.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(15, 25))\nX, y = create_dataset(n_samples=5000, weights=(0.1, 0.2, 0.7), class_sep=0.8)\n\nax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6))\nfor ax, sampler in zip(\n    ax_arr, (NearMiss(version=1), NearMiss(version=2), NearMiss(version=3))\n):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title(\n        f\"Decision function for {sampler.__class__.__name__}-{sampler.version}\"\n    )\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title(f\"Resampling using {sampler.__class__.__name__}-{sampler.version}\")\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``EditedNearestNeighbours`` removes samples of the majority class for which\ntheir class differ from the one of their nearest-neighbors. This sieve can be\nrepeated which is the principle of the\n``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from\nthe ``RepeatedEditedNearestNeighbours`` by changing the $k$ parameter\nof the internal nearest neighors algorithm, increasing it at each iteration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(15, 25))\nX, y = create_dataset(n_samples=500, weights=(0.2, 0.3, 0.5), class_sep=0.8)\n\nax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6))\nfor ax, sampler in zip(\n    ax_arr,\n    (\n        EditedNearestNeighbours(),\n        RepeatedEditedNearestNeighbours(),\n        AllKNN(allow_minority=True),\n    ),\n):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title(f\"Decision function for {sampler.__class__.__name__}\")\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title(f\"Resampling using {sampler.__class__.__name__}\")\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a\nsample should be kept in a dataset or not. The issue is that\n``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy\nsamples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to\nremove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a\n``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3\nnearest-neighbors to remove samples which do not agree with this rule.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(15, 25))\nX, y = create_dataset(n_samples=500, weights=(0.2, 0.3, 0.5), class_sep=0.8)\n\nax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6))\nfor ax, sampler in zip(\n    ax_arr,\n    (\n        CondensedNearestNeighbour(random_state=0),\n        OneSidedSelection(random_state=0),\n        NeighbourhoodCleaningRule(),\n    ),\n):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title(f\"Decision function for {sampler.__class__.__name__}\")\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title(f\"Resampling using {sampler.__class__.__name__}\")\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``InstanceHardnessThreshold`` uses the prediction of classifier to exclude\nsamples. All samples which are classified with a low probability will be\nremoved.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94), class_sep=0.8)\n\nclf = LinearSVC().fit(X, y)\nplot_decision_function(X, y, clf, ax1)\nax1.set_title(f\"Linear SVC with y={Counter(y)}\")\nsampler = InstanceHardnessThreshold(\n    random_state=0,\n    estimator=LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\"),\n)\nclf = make_pipeline(sampler, LinearSVC())\nclf.fit(X, y)\nplot_decision_function(X, y, clf, ax2)\nax2.set_title(f\"Decision function for {sampler.__class__.__name__}\")\nplot_resampling(X, y, sampler, ax3)\nax3.set_title(f\"Resampling using {sampler.__class__.__name__}\")\nfig.tight_layout()\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}