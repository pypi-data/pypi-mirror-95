.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_ensemble_plot_comparison_ensemble_classifier.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_ensemble_plot_comparison_ensemble_classifier.py:


=============================================
Compare ensemble classifiers using resampling
=============================================

Ensembling classifiers have shown to improve classification performance compare
to single learner. However, they will be affected by class imbalance. This
example shows the benefit of balancing the training set before to learn
learners. We are making the comparison with non-balanced ensemble methods.

We make a comparison using the balanced accuracy and geometric mean which are
metrics widely used in the literature to evaluate models learned on imbalanced
set.


.. code-block:: default


    # Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
    # License: MIT

    import itertools

    import matplotlib.pyplot as plt
    import numpy as np

    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import BaggingClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import AdaBoostClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import balanced_accuracy_score

    from imblearn.datasets import fetch_datasets
    from imblearn.ensemble import BalancedBaggingClassifier
    from imblearn.ensemble import BalancedRandomForestClassifier
    from imblearn.ensemble import EasyEnsembleClassifier
    from imblearn.ensemble import RUSBoostClassifier

    from imblearn.metrics import geometric_mean_score


    def plot_confusion_matrix(
        cm,
        classes,
        ax,
        normalize=False,
        title="Confusion matrix",
        cmap=plt.cm.Blues,
    ):
        """
        This function prints and plots the confusion matrix.
        Normalization can be applied by setting `normalize=True`.
        """
        print(cm)
        print("")

        ax.imshow(cm, interpolation="nearest", cmap=cmap)
        ax.set_title(title)
        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes, rotation=45)
        plt.sca(ax)
        plt.yticks(tick_marks, classes)

        fmt = ".2f" if normalize else "d"
        thresh = cm.max() / 2.0
        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
            ax.text(
                j,
                i,
                format(cm[i, j], fmt),
                horizontalalignment="center",
                color="white" if cm[i, j] > thresh else "black",
            )

        ax.set_ylabel("True label")
        ax.set_xlabel("Predicted label")









Load an imbalanced dataset
##############################################################################
 We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
 (number of majority sample for a minority sample). The data are then split
 into training and testing.


.. code-block:: default


    satimage = fetch_datasets()["satimage"]
    X, y = satimage.data, satimage.target
    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)








Classification using a single decision tree
##############################################################################
 We train a decision tree classifier which will be used as a baseline for the
 rest of this example.

The results are reported in terms of balanced accuracy and geometric mean
which are metrics widely used in the literature to validate model trained on
imbalanced set.


.. code-block:: default


    tree = DecisionTreeClassifier()
    tree.fit(X_train, y_train)
    y_pred_tree = tree.predict(X_test)
    print("Decision tree classifier performance:")
    print(
        f"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_tree):.2f} - "
        f"Geometric mean {geometric_mean_score(y_test, y_pred_tree):.2f}"
    )
    cm_tree = confusion_matrix(y_test, y_pred_tree)
    fig, ax = plt.subplots()
    plot_confusion_matrix(
        cm_tree, classes=np.unique(satimage.target), ax=ax, title="Decision tree"
    )




.. image:: /auto_examples/ensemble/images/sphx_glr_plot_comparison_ensemble_classifier_001.png
    :alt: Decision tree
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Decision tree classifier performance:
    Balanced accuracy: 0.75 - Geometric mean 0.72
    [[1384   68]
     [  72   85]]





Classification using bagging classifier with and without sampling
##############################################################################
 Instead of using a single tree, we will check if an ensemble of decsion tree
 can actually alleviate the issue induced by the class imbalancing. First, we
 will use a bagging classifier and its counter part which internally uses a
 random under-sampling to balanced each boostrap sample.


.. code-block:: default


    bagging = BaggingClassifier(n_estimators=50, random_state=0)
    balanced_bagging = BalancedBaggingClassifier(n_estimators=50, random_state=0)

    bagging.fit(X_train, y_train)
    balanced_bagging.fit(X_train, y_train)

    y_pred_bc = bagging.predict(X_test)
    y_pred_bbc = balanced_bagging.predict(X_test)








Balancing each bootstrap sample allows to increase significantly the balanced
accuracy and the geometric mean.


.. code-block:: default


    print("Bagging classifier performance:")
    print(
        f"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_bc):.2f} - "
        f"Geometric mean {geometric_mean_score(y_test, y_pred_bc):.2f}"
    )
    cm_bagging = confusion_matrix(y_test, y_pred_bc)
    fig, ax = plt.subplots(ncols=2)
    plot_confusion_matrix(
        cm_bagging, classes=np.unique(satimage.target), ax=ax[0], title="Bagging"
    )

    print("Balanced Bagging classifier performance:")
    print(
        f"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_bbc):.2f} - "
        f"Geometric mean {geometric_mean_score(y_test, y_pred_bbc):.2f}"
    )
    cm_balanced_bagging = confusion_matrix(y_test, y_pred_bbc)
    plot_confusion_matrix(
        cm_balanced_bagging,
        classes=np.unique(satimage.target),
        ax=ax[1],
        title="Balanced bagging",
    )




.. image:: /auto_examples/ensemble/images/sphx_glr_plot_comparison_ensemble_classifier_002.png
    :alt: Bagging, Balanced bagging
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Bagging classifier performance:
    Balanced accuracy: 0.73 - Geometric mean 0.68
    [[1424   28]
     [  82   75]]

    Balanced Bagging classifier performance:
    Balanced accuracy: 0.86 - Geometric mean 0.86
    [[1324  128]
     [  31  126]]





Classification using random forest classifier with and without sampling
##############################################################################
 Random forest is another popular ensemble method and it is usually
 outperforming bagging. Here, we used a vanilla random forest and its balanced
 counterpart in which each bootstrap sample is balanced.


.. code-block:: default


    rf = RandomForestClassifier(n_estimators=50, random_state=0)
    brf = BalancedRandomForestClassifier(n_estimators=50, random_state=0)

    rf.fit(X_train, y_train)
    brf.fit(X_train, y_train)

    y_pred_rf = rf.predict(X_test)
    y_pred_brf = brf.predict(X_test)

    # Similarly to the previous experiment, the balanced classifier outperform the
    # classifier which learn from imbalanced bootstrap samples. In addition, random
    # forest outsperforms the bagging classifier.

    print("Random Forest classifier performance:")
    print(
        f"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_rf):.2f} - "
        f"Geometric mean {geometric_mean_score(y_test, y_pred_rf):.2f}"
    )
    cm_rf = confusion_matrix(y_test, y_pred_rf)
    fig, ax = plt.subplots(ncols=2)
    plot_confusion_matrix(
        cm_rf, classes=np.unique(satimage.target), ax=ax[0], title="Random forest"
    )

    print("Balanced Random Forest classifier performance:")
    print(
        f"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_brf):.2f} - "
        f"Geometric mean {geometric_mean_score(y_test, y_pred_brf):.2f}"
    )
    cm_brf = confusion_matrix(y_test, y_pred_brf)
    plot_confusion_matrix(
        cm_brf,
        classes=np.unique(satimage.target),
        ax=ax[1],
        title="Balanced random forest",
    )




.. image:: /auto_examples/ensemble/images/sphx_glr_plot_comparison_ensemble_classifier_003.png
    :alt: Random forest, Balanced random forest
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Random Forest classifier performance:
    Balanced accuracy: 0.73 - Geometric mean 0.68
    [[1435   17]
     [  84   73]]

    Balanced Random Forest classifier performance:
    Balanced accuracy: 0.88 - Geometric mean 0.88
    [[1291  161]
     [  19  138]]





Boosting classifier
##############################################################################
 In the same manner, easy ensemble classifier is a bag of balanced AdaBoost
 classifier. However, it will be slower to train than random forest and will
 achieve worse performance.


.. code-block:: default


    base_estimator = AdaBoostClassifier(n_estimators=10)
    eec = EasyEnsembleClassifier(n_estimators=10, base_estimator=base_estimator)
    eec.fit(X_train, y_train)
    y_pred_eec = eec.predict(X_test)
    print("Easy ensemble classifier performance:")
    print(
        f"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_eec):.2f} - "
        f"Geometric mean {geometric_mean_score(y_test, y_pred_eec):.2f}"
    )
    cm_eec = confusion_matrix(y_test, y_pred_eec)
    fig, ax = plt.subplots(ncols=2)
    plot_confusion_matrix(
        cm_eec,
        classes=np.unique(satimage.target),
        ax=ax[0],
        title="Easy ensemble classifier",
    )

    rusboost = RUSBoostClassifier(n_estimators=10, base_estimator=base_estimator)
    rusboost.fit(X_train, y_train)
    y_pred_rusboost = rusboost.predict(X_test)
    print("RUSBoost classifier performance:")
    print(
        f"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred_rusboost):.2f} - "
        f"Geometric mean {geometric_mean_score(y_test, y_pred_rusboost):.2f}"
    )
    cm_rusboost = confusion_matrix(y_test, y_pred_rusboost)
    plot_confusion_matrix(
        cm_rusboost,
        classes=np.unique(satimage.target),
        ax=ax[1],
        title="RUSBoost classifier",
    )

    plt.show()



.. image:: /auto_examples/ensemble/images/sphx_glr_plot_comparison_ensemble_classifier_004.png
    :alt: Easy ensemble classifier, RUSBoost classifier
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Easy ensemble classifier performance:
    Balanced accuracy: 0.85 - Geometric mean 0.85
    [[1225  227]
     [  23  134]]

    RUSBoost classifier performance:
    Balanced accuracy: 0.85 - Geometric mean 0.85
    [[1212  240]
     [  20  137]]

    /home/glemaitre/Documents/packages/imbalanced-learn/examples/ensemble/plot_comparison_ensemble_classifier.py:245: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.
      plt.show()





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  16.755 seconds)

**Estimated memory usage:**  203 MB


.. _sphx_glr_download_auto_examples_ensemble_plot_comparison_ensemble_classifier.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_comparison_ensemble_classifier.py <plot_comparison_ensemble_classifier.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_comparison_ensemble_classifier.ipynb <plot_comparison_ensemble_classifier.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
