
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=cp1252" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Embedding</title>
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="next" title="Image Grid" href="imagegrid.html" />
    <link rel="prev" title="Image Viewer" href="imageviewer.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="imagegrid.html" title="Image Grid"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="imageviewer.html" title="Image Viewer"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Orange3 Image Analytics  documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Image Embedding</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
          <div class="body" role="main">
            
  <div class="section" id="image-embedding">
<h1>Image Embedding</h1>
<p>Image embedding through deep neural networks.</p>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li>Images: List of images.</li>
</ul>
<p><strong>Outputs</strong></p>
<ul class="simple">
<li>Embeddings: Images represented with a vector of numbers.</li>
<li>Skipped Images: List of images where embeddings were not calculated.</li>
</ul>
<p><strong>Image Embedding</strong> reads images and uploads them to a remote server or evaluate them locally. Deep learning models are used to calculate a feature vector for each image. It returns an enhanced data table with additional columns (image descriptors).</p>
<p>Images can be imported with [Import Images](importimages.md) widget or as paths to images in a spreadsheet. In this case the column with images paths needs a three-row header with <em>type=image</em> label in the third row.</p>
<p>![](images/header-example.png)</p>
<p>Image Embedding offers several embedders, each trained for a specific task. Images are sent to a server or they are evaluated locally on the user’s computer, where vectors representations are computed. SqueezeNet embedder offers a fast evaluation on users computer which does not require an internet connection. If you decide to use other embedders than SqueezeNet, you will need an internet connection. Images sent to the server are not stored anywhere.</p>
<p>![](images/ImageEmbedding-stamped.png)</p>
<ol class="arabic">
<li><p class="first">Information on the number of embedded images and images skipped.</p>
</li>
<li><p class="first">Settings:
- <em>Image attribute</em>: attribute containing images you wish to embed
- <em>Embedder</em>:</p>
<blockquote>
<div><ul class="simple">
<li>SqueezeNet: [Small and fast](<a class="reference external" href="https://arxiv.org/abs/1602.07360" target="_blank">https://arxiv.org/abs/1602.07360</a>) model for image recognition trained on ImageNet.</li>
<li>Inception v3: [Google’s Inception v3](<a class="reference external" href="https://arxiv.org/abs/1512.00567" target="_blank">https://arxiv.org/abs/1512.00567</a>) model trained on ImageNet.</li>
<li>VGG-16: [16-layer image recognition model](<a class="reference external" href="https://arxiv.org/abs/1409.1556" target="_blank">https://arxiv.org/abs/1409.1556</a>) trained on ImageNet.</li>
<li>VGG-19: [19-layer image recognition model](<a class="reference external" href="https://arxiv.org/abs/1409.1556" target="_blank">https://arxiv.org/abs/1409.1556</a>) trained on ImageNet.</li>
<li>Painters: A model trained to [predict painters from artwork images](<a class="reference external" href="http://blog.kaggle.com/2016/11/17/painter-by-numbers-competition-1st-place-winners-interview-nejc-ilenic/" target="_blank">http://blog.kaggle.com/2016/11/17/painter-by-numbers-competition-1st-place-winners-interview-nejc-ilenic/</a>).</li>
<li>DeepLoc: A model trained to analyze [yeast cell images](<a class="reference external" href="https://www.ncbi.nlm.nih.gov/pubmed/29036616" target="_blank">https://www.ncbi.nlm.nih.gov/pubmed/29036616</a>).</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Tick the box on the left to start the embedding automatically. Alternatively, click <em>Apply</em>. To cancel the embedding, click <em>Cancel</em>.</p>
</li>
<li><p class="first">Access help.</p>
</li>
</ol>
<div class="section" id="embedders">
<h2>Embedders</h2>
<p><strong>InceptionV3</strong> is Google’s deep neural network for image recognition. It is trained on the ImageNet data set. The model we are using is available [here](<a class="reference external" href="http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz" target="_blank">http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz</a>). For the embedding, we use the activations of the penultimate layer of the model, which represents images with vectors.</p>
<p><strong>SqueezeNet</strong> is a deep model for image recognition that achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. The model is trained on the ImageNet dataset. We re-implemented the SqueezeNet by using weights from the [author’s pretrained model](<a class="reference external" href="https://github.com/DeepScale/SqueezeNet" target="_blank">https://github.com/DeepScale/SqueezeNet</a>). We use activations from pre-softmax (<cite>flatten10</cite>) layer as an embedding.</p>
<p><strong>VGG16</strong> and <strong>VGG19</strong> are deep neural networks for image recognition proposed by Visual Geometry Group from the University of Oxford. They are trained on the ImageNet data set. We use a [community implementation](<a class="reference external" href="https://github.com/machrisaa/tensorflow-vgg" target="_blank">https://github.com/machrisaa/tensorflow-vgg</a>) of networks with original weights. As an embedding, we use activations of the penultimate layer - <cite>fc7</cite>.</p>
<p>Image Embedding also includes [<strong>Painters</strong>](<a class="reference external" href="https://github.com/inejc/painters" target="_blank">https://github.com/inejc/painters</a>), an embedder that was trained on 79,433 images of paintings by 1,584 painters and won Kaggle’s Painter by Numbers competition. Activations of the penultimate layer of the network are used as an embedding.</p>
<p><strong>DeepLoc</strong> is a convolutional network trained on 21,882 images of single cells that were manually assigned to one of 15 localization compartments. We use the pre-trained network proposed by [authors](<a class="reference external" href="https://github.com/okraus/DeepLoc" target="_blank">https://github.com/okraus/DeepLoc</a>). The embeddings are activations of penultimate layer <cite>fc_2</cite>.</p>
<p>An [article](<a class="reference external" href="https://www.nature.com/articles/s41467-019-12397-x" target="_blank">https://www.nature.com/articles/s41467-019-12397-x</a>) by Godec et al. (2019) explains how the embeddings work and how to use it in Orange.</p>
</div>
<div class="section" id="example">
<h2>Example</h2>
<p>Let us first import images from a folder with [Import Images](importimages.md). We have three images of an orange, a banana and a strawberry in a folder called Fruits. From <strong>Import Images</strong> we will send a data table containing a column with image paths to <strong>Image Embedding</strong>.</p>
<p>We will use the default embedder <em>SqueezeNet</em>. The widget will automatically start retrieving image vectors from the server.</p>
<p>![](images/ImageEmbedding-Example1.png)</p>
<p>Once the computation is done, you can observe the enhanced data in a <strong>Data Table</strong>. With the retrieved embeddings, you can continue with any machine learning method Orange offers. Below is an example for clustering.</p>
<p>![](images/ImageEmbedding-Example2.png)</p>
</div>
</div>


            <div class="clearer"></div>
          </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="imagegrid.html" title="Image Grid"
             >next</a> |</li>
        <li class="right" >
          <a href="imageviewer.html" title="Image Viewer"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Orange3 Image Analytics  documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Image Embedding</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2016, Laboratory of Bioinformatics, Faculty of Computer Science, University of Ljubljana.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.1.2.
    </div>
  </body>
</html>