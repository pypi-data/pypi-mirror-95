{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lightning intro to sensAI\n",
    "\n",
    "In this notebook we will demonstrate some of sensAI's main features by training a model together\n",
    "with feature extractors and custom normalization rules. This will also demonstrate how easy it is to wrap one's\n",
    "own model declaration into a sensAI model."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Before running the notebook\n",
    "\n",
    "Install the package and its dependencies, if you haven't done so already. E.g. for an editable install call\n",
    "```\n",
    "pip install -e .\n",
    "```\n",
    "from the root directory. You can also execute this command directly in the notebook but will need to reload the\n",
    "kernel afterwards"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Note - this cell should be executed only once per session\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# in order to get the top level modules; they are not part of the package\n",
    "os.chdir(\"..\")\n",
    "sys.path.append(os.path.abspath(\".\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sensai as sn\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "from sensai import VectorRegressionModel\n",
    "from sensai.data_transformation import DFTNormalisation\n",
    "from sensai.evaluation.eval_util import createVectorModelEvaluator\n",
    "from sensai.data import InputOutputData\n",
    "from sensai.tracking.clearml_tracking import ClearMLExperiment\n",
    "import sensai.featuregen as fgen\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import get_config\n",
    "\n",
    "c  = get_config(reload=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "housing_data = c.datafile_path(\"boston_housing.csv\", stage=c.RAW)\n",
    "housing_df = pd.read_csv(housing_data)\n",
    "\n",
    "housing_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = housing_df.copy()\n",
    "y = pd.DataFrame({\"nox\": X.pop(\"nox\")})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"We will use this as target\")\n",
    "y.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a Custom Model\n",
    "\n",
    "Although sensAI provides several implementations of different models across major frameworks (SKlearn, TensorFlow,\n",
    "PyTorch), we put special care to make it easy for you to bring your own model. The `VectorModel` based\n",
    "classes provides abstractions which can be used for most learning problems of the type \"datapoint in,\n",
    "row of predictions out\". The row of predictions can contain a vector with class probabilities, one or multiple\n",
    "regression targets and so on. For problems of the type: \"datapoint in, multidimensional tensor out\", see the\n",
    "tutorial in TBA.\n",
    "\n",
    "We will use VectorModel to wrap scikit-learn's implementation of a multi layer perceptron."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomModel(VectorRegressionModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = MLPRegressor()\n",
    "\n",
    "    def _predict(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        values = self.model.predict(x)\n",
    "        return pd.DataFrame({\"nox\": values}, index=x.index)\n",
    "\n",
    "    def _fit(self, X: pd.DataFrame, Y: pd.DataFrame):\n",
    "        self.model.fit(X, Y.values.ravel())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Generation and Normalization\n",
    "\n",
    "Some of sensAI's core design principles include explicitness and safety of data wrangling. Special care was taken to\n",
    "ensure that everything that happens to input data during feature extraction, preprocessing, training and inference was\n",
    "intended by the user. Since for many projects feature engineering is decisive for model performance, it is absolutely\n",
    "crucial that the developer has full control over all transformations that are going on during training and inference.\n",
    "\n",
    "\n",
    "The feature generation and normalization modules helps with this, allowing fine-grained control over each step in the\n",
    "processing pipeline. Since the feature generators and the normalization data frame transforms can be bound to a sensAI\n",
    "model, it is guaranteed that the data pipeline at inference will work exactly as intended.\n",
    "If something unexpected happens at inference time, like an unknown column, wrong order of columns etc, an error will be\n",
    "raised. Errors will also be raised (unless specifically disabled) if there are columns for which no normalization rules\n",
    " have been provided for columns.\n",
    "This ensures that the user has thought about how to deal with different features and that no surprises happen.\n",
    "\n",
    "This level of control comes at the price of verbosity. sensAI classes and arguments tend to have long names,\n",
    "explaining exactly what they do and what the intended use case looks like.\n",
    "\n",
    "Below we will show an example of feature engineering.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining Feature Generators\n",
    "\n",
    "Below we will define two feature generators. One will compensate the tax for fraud, by assuming that if the declared\n",
    "tax in the dataframe is above a threshold, we have to subtract some fixed value that was lied about. The threshold\n",
    "is extracted from the dataframe when the feature generator is fit.\n",
    "\n",
    "The second feature generator simply takes the columns \"crim\" and \"age\" as is and marks that they should be normalized."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TaxFraudFeaturegen(fgen.FeatureGenerator):\n",
    "    def __init__(self, tax_column=\"tax\", value_lied_about=12.0):\n",
    "        self.value_lied_about = value_lied_about\n",
    "        self.tax_column = tax_column\n",
    "        self.threshold = None\n",
    "        super().__init__(\n",
    "            normalisationRuleTemplate=DFTNormalisation.RuleTemplate(\n",
    "                transformer=MinMaxScaler()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _fit(self, X: pd.DataFrame, Y: pd.DataFrame, ctx=None):\n",
    "        self.threshold = X[self.tax_column].median()\n",
    "\n",
    "    def compensate_for_fraud(self, tax: float):\n",
    "        if tax > self.threshold:\n",
    "            tax = tax - self.value_lied_about\n",
    "        return tax\n",
    "\n",
    "    def _generate(self, df: pd.DataFrame, ctx=None) -> pd.DataFrame:\n",
    "        result = pd.DataFrame()\n",
    "        result[self.tax_column] = df[self.tax_column].apply(self.compensate_for_fraud)\n",
    "        return result\n",
    "\n",
    "\n",
    "crime_age_featuregen = fgen.FeatureGeneratorTakeColumns(\n",
    "    columns=[\"crim\", \"age\"],\n",
    "    normalisationRuleTemplate=DFTNormalisation.RuleTemplate(skip=True),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Feature Registry\n",
    "\n",
    "We could simply take the feature generators as they are and plug them into our model but instead we demonstrate\n",
    "one more class in sensAI: the feature registry. Creating a registry is convenient for rapid experimentation\n",
    "and for keeping track of useful features in a large project. You might not know which ones will be useful for which\n",
    "model so the registry abstraction helps you checking in features into git and staying organized.\n",
    "\n",
    "Here we create the a dedicated registry for the housing features. The registry will hold factories\n",
    "of featuregens which will create singleton instances if called withing the training/inference pipeline\n",
    "(this is optional).\n",
    "The collector is pinned to a registry and allows to call the registered features by name (if desired).\n",
    "This might not make much sense in a notebook but imagine having a central feature registry somewhere in you code. This\n",
    "way you can combine the registered features with some features that you cooked up in a script, all in a few lines of code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "housing_feature_registry = fgen.FeatureGeneratorRegistry(useSingletons=True)\n",
    "\n",
    "housing_feature_registry.tax = TaxFraudFeaturegen\n",
    "\n",
    "feature_collector = fgen.FeatureCollector(\"tax\", crime_age_featuregen, registry=housing_feature_registry)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalization of Input and Target\n",
    "\n",
    "Now we come to the issue of normalization. In each feature generator we have declared how the resulting\n",
    "columns should be normalized. We can use this information by instantiating `DFTNormalisation`.\n",
    "If a rule for some column is missing, the normalization object will raise an error. There is a way\n",
    "to circumvent this error - set `requireAllHandled` to False. In that case, you should probably\n",
    "use a defaultTransformerFactory to normalize all remaining columns. However, we recommend to explicitly pass\n",
    "all normalization rules to the feature generators themselves, just to be sure that nothing is missing.\n",
    "\n",
    "For normalizing the target we have to use an invertible transformer, we will take the MaxAbsScaler here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dft_normalisation = sn.data_transformation.DFTNormalisation(\n",
    "    feature_collector.getNormalizationRules(),\n",
    "    requireAllHandled=True,\n",
    ")\n",
    "\n",
    "target_transformer = sn.data_transformation.DFTSkLearnTransformer(MaxAbsScaler())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combining Everything with the Model\n",
    "\n",
    "Now we can plug all these components into our vector model and enjoy a safe and robust that will\n",
    "work during training and inference. The model already has methods for saving and loading and is ready to\n",
    "be deployed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_model = CustomModel()\n",
    "\n",
    "custom_model = custom_model \\\n",
    "    .withFeatureCollector(feature_collector) \\\n",
    "    .withInputTransformers(dft_normalisation) \\\n",
    "    .withTargetTransformer(target_transformer) \\\n",
    "    .withName(\"housing_predictor\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_model.fit(X, y)\n",
    "custom_model.predict(X).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SensAI Featuregen vs. Sklearn Pipelines\n",
    "\n",
    "TBA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation\n",
    "\n",
    "sensAI has extensive support for evaluating different types of models with different methods, including\n",
    "cross validation. The evaluation has native support for experiment tracking frameworks, like clearML or MLflow.\n",
    "Here we will use clearML, so after running this notebook you will be able to see the result in the\n",
    "clearML demo-server.\n",
    "\n",
    "The evaluation is generally based on the following structure: an `Evaluator` object holds a dataset.\n",
    "An `Evaluator` can evaluate multiple models by calling `Evaluator.evalModel(model)`,\n",
    "this ensures that the same kind of evaluation is performed and thus the results can be compared in meaningful way\n",
    "(the latter is crucial for model selection). This `.evalModel(model)` call returns an EvalData object, h\n",
    "olding the evaluation data and containing methods for computing metrics and visualization.\n",
    "\n",
    "Similarly, a `CrossValidator` holds data and can perform cross validation,\n",
    "\n",
    "Below we will show a simple example for that, using the lower-level evaluation interfaces. There is also\n",
    "a higher level evaluation interfaces in the eval_util module, we will leave that to a separate intro."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "io_data = InputOutputData(X, y)\n",
    "\n",
    "clearml_experiment = ClearMLExperiment(projectName=\"sensai_demo\", taskName=\"custom_model\")\n",
    "evaluator = createVectorModelEvaluator(io_data, isRegression=custom_model.isRegressionModel(),\n",
    "                                       testFraction=0.2)\n",
    "evaluator.setTrackedExperiment(clearml_experiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_custom_model = CustomModel() \\\n",
    "    .withFeatureCollector(feature_collector) \\\n",
    "    .withInputTransformers(dft_normalisation) \\\n",
    "    .withTargetTransformer(target_transformer) \\\n",
    "    .withName(\"housing_predictor\")\n",
    "\n",
    "evaluator.fitModel(new_custom_model)\n",
    "eval_stats = evaluator.evalModel(new_custom_model).getEvalStats()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(eval_stats.getAll())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_stats.plotScatterGroundTruthPredictions()\n",
    "eval_stats.plotErrorDistribution()\n",
    "eval_stats.plotHeatmapGroundTruthPredictions()\n",
    "plt.show()\n",
    "print(\"Demonstrating plotting capabilities\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_stats.plotScatterGroundTruthPredictions()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Things we left out in this intro\n",
    "\n",
    " - Already implemented models and feature generators\n",
    " - Caching (this is actually one of the central features)\n",
    " - Support for ensembling and parallelization\n",
    " - The local search and hyperopt modules, including grid-search, simulated-annealing and other stuff\n",
    " - kNN and clustering implementations\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}