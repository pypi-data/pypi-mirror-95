{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluating clustering algorithms\n",
    "\n",
    "The present library contains utilities for evaluating different clustering algorithms\n",
    "(with or without ground truth labels). On top of the evaluation utilities there are classes for\n",
    "performing parameters sweeps and model selection. Here we give an overview of the most important functionality\n",
    "\n",
    "\n",
    "## Before running the notebook\n",
    "\n",
    "Install the library and its dependencies with, if you haven't done so already\n",
    "```\n",
    "pip install -e .\n",
    "```\n",
    "from the root directory. You can also execute this command directly in the notebook but will need to reload the\n",
    "kernel afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Note - this cell should be executed only once per session\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# in order to get the config, it is not part of the library\n",
    "os.chdir(\"..\")\n",
    "sys.path.append(os.path.abspath(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import DBSCAN\n",
    "import seaborn as sns\n",
    "import geopandas as gp\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "from sensai.clustering.coordinate_clustering import SKLearnCoordinateClustering\n",
    "from sensai.hyperopt import GridSearch\n",
    "from sensai.evaluation.evaluator_clustering import ClusteringModelSupervisedEvaluator, \\\n",
    "    ClusteringModelUnsupervisedEvaluator\n",
    "from sensai.evaluation.eval_stats import ClusteringUnsupervisedEvalStats, ClusteringSupervisedEvalStats, \\\n",
    "    AdjustedMutualInfoScore\n",
    "from sensai.evaluation.clustering_ground_truth import PolygonAnnotatedCoordinates\n",
    "\n",
    "from config import get_config\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loading data and config\n",
    "c  = get_config(reload=True)\n",
    "sampleFile = c.datafile_path(\"sample\", stage=c.RAW) # this can point to a directory or a shp/geojson file\n",
    "coordinatesDF = gp.read_file(sampleFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluating a single model\n",
    "\n",
    "For a single model that was already fitted, evaluation statistics can be extracted with `ClusteringEvalStats`, see the\n",
    "example below (the eval_stats object can also be used to retrieve evaluation results one by one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dbscan = SKLearnCoordinateClustering(DBSCAN(eps=150, min_samples=20))\n",
    "dbscan.fit(coordinatesDF)\n",
    "evalStats = ClusteringUnsupervisedEvalStats.fromModel(dbscan)\n",
    "pprint(evalStats.getAll())\n",
    "plt.hist(evalStats.clusterSizeDistribution)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model selection\n",
    "\n",
    "For model selection we need to compare different (or differently parametrized) models that were\n",
    "trained on the same dataset. The `ClusteringEvaluator` abstraction was designed with this goal in mind.\n",
    "The evaluator can be used to obtain evaluation statistics for different models that are guaranteed\n",
    "to be comparable with each other (always computed by the same object in the same way). Here an example evaluating\n",
    "a dbscan performance on metrics that don't necessitate ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "modelEvaluator = ClusteringModelUnsupervisedEvaluator(coordinatesDF)\n",
    "\n",
    "dbscanEvalStats = modelEvaluator.evalModel(dbscan, fit=False)  # dbscan was already fitted on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"dbscan_performance: \\n\")\n",
    "pprint(dbscanEvalStats.getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One of the main purposes of evaluators is to be used within classes that perform a parameter sweep, e.g.\n",
    "a `GridSearch`. All such objects return a data frame and (optionally but recommended!) persist all evaluation results\n",
    "in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parameterOptions = {\n",
    "    \"min_samples\": [10, 20],\n",
    "    \"eps\": [50, 150]\n",
    "}\n",
    "\n",
    "# for running the grid search in multiple processes, all objects need to be picklable.\n",
    "# Therefore we pass a named function as model factory instead of a lambda\n",
    "def dbscanFactory(**kwargs):\n",
    "    return SKLearnCoordinateClustering(DBSCAN(**kwargs))\n",
    "\n",
    "dbscanGridSearch = GridSearch(dbscanFactory, parameterOptions, csvResultsPath=os.path.join(c.temp, \"dbscanGridSearchCsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# the results of the grid-search are saved as csv under the path provided above\n",
    "resultDf = dbscanGridSearch.run(modelEvaluator, sortColumnName=\"numClusters\", ascending=False)\n",
    "resultDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The resulting data frame can be used to visualize the results through standard techniques,\n",
    "e.g. pivoting and heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"calinskiHarabaszScores\")\n",
    "chScoreHeatmap = resultDf.pivot(index=\"min_samples\", columns=\"eps\", values=\"CalinskiHarabaszScore\")\n",
    "sns.heatmap(chScoreHeatmap, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"daviesBouldinScores\")\n",
    "chScoreHeatmap = resultDf.pivot(index=\"min_samples\", columns=\"eps\", values=\"DaviesBouldinScore\")\n",
    "sns.heatmap(chScoreHeatmap, cmap=sns.cm.rocket_r, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"numClusters\")\n",
    "numClustersHeatmap = resultDf.pivot(index=\"min_samples\", columns=\"eps\", values=\"numClusters\").astype(int)\n",
    "sns.heatmap(numClustersHeatmap, annot=True)  # something goes wrong with the datatype here, maybe b/c of zero clusters\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dealing with ground truth labels\n",
    "\n",
    "\n",
    "The evaluation classes can take ground truth labels for all coordinates and use them for calculating related metrics.\n",
    "However, such labels are typically hard to come by, especially if the coordinates cover a large area. Therefore the\n",
    "library includes utilities for extracting labels from ground truth provided in form of __cluster polygons in a selected\n",
    "region__. The central class for dealing with this kind of data is `ground_truth.PolygonAnnotatedCoordinates`,\n",
    "see examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The polygons can be read directly from a file, see the documentation for more details\n",
    "groundTruthClusters = PolygonAnnotatedCoordinates(coordinatesDF, c.datafile_path(\"sample\", stage=c.GROUND_TRUTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As usual, the object has methods for plotting and exporting to geodata frames.\n",
    "These can be very useful for inspecting the provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "groundTruthClusters.plot(markersize=0.2, cmap=\"plasma\")\n",
    "plt.show()\n",
    "\n",
    "groundTruthClusters.toGeoDF().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can extract the coordinates and labels for the annotated region and use them in evaluation. In the following\n",
    "we will train our own adaption of DBSCAN, namely `boundedDBSCAN` on datapoints in the ground truth region and\n",
    "evaluate the results against the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "boundedDbscan = SKLearnCoordinateClustering(DBSCAN(eps=150, min_samples=20), minClusterSize=100)\n",
    "groundTruthCoordinates, groundTruthLabels = groundTruthClusters.getCoordinatesLabels()\n",
    "supervisedEvaluator = ClusteringModelSupervisedEvaluator(groundTruthCoordinates, trueLabels=groundTruthLabels)\n",
    "supervisedEvalStats = supervisedEvaluator.evalModel(boundedDbscan)\n",
    "\n",
    "print(\"Supervised evaluation metrics of bounded dbscan:\")\n",
    "pprint(supervisedEvalStats.getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Unsupervised evaluation metrics of bounded dbscan:\")\n",
    "pprint(ClusteringUnsupervisedEvalStats(groundTruthCoordinates, groundTruthLabels).getAll())\n",
    "print(\"\")\n",
    "print(\"Unsupervised evaluation metrics of annotated data\")\n",
    "pprint(ClusteringUnsupervisedEvalStats.fromModel(boundedDbscan).getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The bounded dbscan is performing quite OK with the given parameters, although we see that it segregates clusters too\n",
    "much and has a general tendency towards smaller clusters. These tendencies can be seen visually by comparing the ground\n",
    "truth and the bounded dbscan cluster plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "groundTruthClusters.plot(markersize=0.2, cmap=\"plasma\", includeNoise=False)\n",
    "\n",
    "boundedDbscan.plot(markersize=0.2, includeNoise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Supervised parameter estimation\n",
    "\n",
    "We can now bring everything together by running a grid search and evaluating against ground truth. Very little code\n",
    "is needed for that, so we will write it entirely in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parameterOptions = {\n",
    "    \"min_samples\": [19, 20, 21],\n",
    "    \"eps\": [140, 150, 160]\n",
    "}\n",
    "\n",
    "supervisedGridSearch = GridSearch(dbscanFactory, parameterOptions,\n",
    "                                           csvResultsPath=os.path.join(c.temp, \"bounded_dbscan_grid_search.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we will sort the results by mutual information store\n",
    "supervisedResultDf = supervisedGridSearch.run(supervisedEvaluator, sortColumnName=AdjustedMutualInfoScore.name,\n",
    "                                              ascending=False)\n",
    "supervisedResultDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It seems like we were lucky to already have picked the optimal parameters for the dbscan above.\n",
    "It is also interesting to notice that the supervised scores are in\n",
    "stark disagreement with the unsupervised ones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}