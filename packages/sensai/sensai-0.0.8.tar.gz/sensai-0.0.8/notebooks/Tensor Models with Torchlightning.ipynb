{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tensor Models with PyTorch-Lightning\n",
    "\n",
    "In this notebook we show how sensAI's TensorModel wrappers can be used together with pytorch-lightning models\n",
    "and trainers for even faster development and experimentation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Before running the notebook\n",
    "\n",
    "Install the package and its dependencies, if you haven't done so already. E.g. for an editable install call\n",
    "```\n",
    "pip install -e .\n",
    "```\n",
    "from the root directory. You can also execute this command directly in the notebook but will need to reload the\n",
    "kernel afterwards\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - this cell should be executed only once per session\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# in order to get the top level modules; they are not part of the package\n",
    "os.chdir(\"..\")\n",
    "sys.path.append(os.path.abspath(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sensai.data import InputOutputArrays, DataSplitterFractional\n",
    "\n",
    "from sensai.pytorch_lightning import PLTensorToScalarClassificationModel\n",
    "from sensai.tensor_model import extractArray\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from config import get_config\n",
    "\n",
    "c  = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the Data\n",
    "\n",
    "Unlike in the mnist-based torch-lightning tutorial, here we will load the data in a more \"realistic\" way,\n",
    "namely with pandas from disc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(c.datafile_path(\"mnist_train.csv.zip\"))\n",
    "labels = pd.DataFrame(X.pop(\"label\"))\n",
    "X = X.values.reshape(len(X), 28, 28) / 2 ** 8\n",
    "X = pd.DataFrame({\"mnist_image\": list(X)}, index=labels.index)\n",
    "\n",
    "display(X.head())\n",
    "display(labels.head())\n",
    "\n",
    "display(\"Plotting some image from the data set\")\n",
    "some_image = X.iloc[13, 0]\n",
    "plt.imshow(some_image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Data Loaders in pure PyTorch Lightning\n",
    "\n",
    "First, let us see how training would proceed in pure pytorch-lightning.\n",
    "\n",
    "We will use sensaAI only for obtaining torch data loaders (which otherwise would require a few more lines of code)\n",
    "by transforming the data frames to arrays, splitting them and converting them to loaders."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "VALIDATION_FRACTION = 0.1\n",
    "\n",
    "full_ds = InputOutputArrays(extractArray(X), extractArray(labels))\n",
    "splitter = DataSplitterFractional(1-VALIDATION_FRACTION)\n",
    "\n",
    "train_ds, val_ds = splitter.split(full_ds)\n",
    "train_dataloader = train_ds.toTorchDataLoader()\n",
    "val_dataloader = val_ds.toTorchDataLoader()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the data loaders, let us forget about sensAI for the moment. We create the model declaration and\n",
    "trainer with pytorch-lightning and fit on the MNIST data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MNISTModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.float()\n",
    "        x = torch.relu(self.l1(x.view(x.size(0), -1)))\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, *args):\n",
    "        x, y = batch\n",
    "        loss = F.cross_entropy(self(x), y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args):\n",
    "        x, y = batch\n",
    "        loss = F.cross_entropy(self(x), y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mnist_model = MNISTModel()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=3, progress_bar_refresh_rate=20)\n",
    "trainer.fit(mnist_model, train_dataloader, val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us pick some images from the validation set and look at the results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mini_test_set = val_dataloader.dataset[10:20]\n",
    "test_images, test_labels = mini_test_set\n",
    "\n",
    "display(mnist_model(test_images).argmax(axis=1))\n",
    "display(test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrapping the Model with sensAI\n",
    "\n",
    "Now let us wrap the model with sensAI interfaces. Since sensAI offers dedicated wrappers\n",
    "for pytorch-lightning models, this requires only one additional line of code.\n",
    "\n",
    "This model maps a tensor to a single label, so the correct class to wrap it with is `PLTensorToScalarClassificationModel`,\n",
    "where the `PL` prefix stands for pytorch-lightning."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mnist_model = MNISTModel()\n",
    "trainer = pl.Trainer(max_epochs=3, progress_bar_refresh_rate=20)\n",
    "sensaiMnistModel = PLTensorToScalarClassificationModel(mnist_model, trainer, validationFraction=VALIDATION_FRACTION)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NB: Even without dedicated wrappers, it would require only a few more lines of code to get a custom implementation of\n",
    "a suitable sensAI base class that wraps one's model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the wrapped model, we can fit directly on the data frames. We don't lose any of the niceties that pytorch-lightning\n",
    "brings to the game (both the original model and the trainer are available in `sensaiMnistModel`). By wrapping the\n",
    "model and trainer we gain all the safety, transparency, flexibility in feature engineering as well\n",
    "as extensive support for model evaluation that sensAI is all about."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sensaiMnistModel.fit(X, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The wrapped model performs predictions on data frames. Let us take some points from the training set,\n",
    "perform a prediction on them and have a look at the true labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(\"Predicted data frame\")\n",
    "display(sensaiMnistModel.predict(X.iloc[:10]))\n",
    "display(\"True labels data frame\")\n",
    "display(labels.iloc[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating Tensor Models\n",
    "\n",
    "TODO - the evaluation part is unfinished yet (although we could already the above classifier with the standard\n",
    "vector model evaluators).\n",
    "We should also include TensorToTensor models here and show how to evaluate them\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}