{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training by backpropagation through time (BPTT)\n",
    "\n",
    "BPTT is normally a procedure used while training recurrent neural networks. In the case of spiking networks, even if the network is not recurrent, it has a memory of its previous processing steps through the persistence of membrane potentials. Unlike normal neural networks, spiking networks have an internal state that lasts in time.\n",
    "\n",
    "This is why BPTT can be used for more precise (but also much more computationally expensive) training in SNNs  for sequential tasks.\n",
    "In sinabs, backpropagation in the spiking network is accomplished through a surrogate gradient method, since the spiking nonlinearity is not differentiable.\n",
    "\n",
    "In this notebook, we will train a spiking network directly (without training an analog network first), on the Sequential MNIST task. In Sequential MNIST, a network is shown the 28x28-pixel MNIST digits one row after the other. The input to the network is a single row of 28 pixels, followed by the second one, etc, until all 28 rows are shown. At this point, the network makes a prediction on the digit label.\n",
    "\n",
    "First, we define the MNIST dataset. Note that pixel values are between 0 and 1. We turn those values into probabilities of spiking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class MNIST_Dataset(datasets.MNIST):\n",
    "    def __init__(self, root, train=True, single_channel=False):\n",
    "        datasets.MNIST.__init__(self, root, train=train, download=True)\n",
    "        self.single_channel = single_channel\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = img.float() / 255.\n",
    "        \n",
    "        # default is  by row, output is [time, channels] = [28, 28]\n",
    "        # OR if we want by single item, output is [784, 1]\n",
    "        if self.single_channel:\n",
    "            img = img.reshape(-1).unsqueeze(1)\n",
    "        \n",
    "        spikes = torch.rand(size=img.shape) < img\n",
    "        spikes = spikes.float()\n",
    "        \n",
    "        return spikes, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset_test = MNIST_Dataset(root=\"./data/\", train=False)\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=BATCH_SIZE, drop_last=True)\n",
    "\n",
    "dataset = MNIST_Dataset(root=\"./data/\", train=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a baseline\n",
    "\n",
    "We must demonstrate that this task is not solvable with similar accuracy by a memory-less analog network, despite being sequential. Let us then try to train such a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "ann = nn.Sequential(\n",
    "    nn.Linear(28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e42308f429c49fab9f8c1aea0f8e6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=937.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5853f477184499c8a8ffb87e7b9eeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=937.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a054c047e04941809230a0ca4fd977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=937.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61f70fea8e54494bad38454a42d0488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=937.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b503503920fd44e88c3f3fe904ea8598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=937.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ann.parameters())\n",
    "\n",
    "for epoch in range(5):\n",
    "    pbar = tqdm(dataloader)\n",
    "    for img, target in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target = target.unsqueeze(1).repeat([1, 28])\n",
    "        img = img.reshape([-1, 28])\n",
    "        target = target.reshape([-1])\n",
    "\n",
    "        out = ann(img)\n",
    "#         out = out.sum(1)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5c7199c64d4a838835615a648c1909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.4740584935897436\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "\n",
    "pbar = tqdm(dataloader_test)\n",
    "for img, target in pbar:\n",
    "    \n",
    "    img = img.reshape([-1, 28])\n",
    "    out = ann(img)\n",
    "    out = out.reshape([64, 28, 10])\n",
    "    out = out.sum(1)\n",
    "\n",
    "    predicted = torch.max(out, axis=1)[1]\n",
    "    acc = (predicted == target).sum().numpy() / BATCH_SIZE\n",
    "    accs.append(acc)\n",
    "        \n",
    "print(sum(accs)/len(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a spiking network\n",
    "\n",
    "We then define a 4-layer fully connected spiking neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinabs.from_torch import from_model\n",
    "\n",
    "model = from_model(ann, batch_size=BATCH_SIZE).to(device)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Here, we begin training. Note that the state of the network *must* be reset at every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch convolutional layers don't support inputs that aren't 4-dimensional (batch, channels, height, width).\n",
    "\n",
    "As a workaround, when using sinabs, you'll have to *squeeze the time and batch dimensions*. Starting from data in the form (batch, time, channels, ...), the data should be squeezed to (batch*time, channels, ...).\n",
    "\n",
    "The spiking layer will automatically unpack that dimension distinguishing between batch and time, provided this convention is followed exactly, and the `batch_size=...` parameter is correctly defined for the spiking model.\n",
    "\n",
    "Here, we can see that the input dimensionality is (64, 28, 28) = (batch, time, channels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for img in dataloader:\n",
    "    print(img[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this reason, in the training we included a `reshape(-1, 28)` (to squeeze the dimensions in input)\n",
    "and a `reshape((BATCH_SIZE, 28, 10))` on the output to restore the original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1833ee2a5c5d4bc18d4f703a8a184aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=937.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a2d224136b440e8dfd9af762816f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=937.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e5608af6854dd4aeda3fc69f9e5415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=937.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b35740c44840f4a76f653ebd23245b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=937.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f1e61286fd4b5c8f0ea0da6b99405f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=937.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(5):\n",
    "    pbar = tqdm(dataloader)\n",
    "    for img, target in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        model.reset_states()\n",
    "\n",
    "        img = img.reshape((-1, 28))  # merging time and batch dimensions\n",
    "        out = model.spiking_model(img.to(device))\n",
    "        out = out.reshape((BATCH_SIZE, 28, 10))  # restoring original dimensions\n",
    "        \n",
    "        # the output of the network is summed over the 28 time steps (rows)\n",
    "        out = out.sum(1)\n",
    "        loss = criterion(out, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaeabcf81aa544ad94da0d110338fcd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.8376402243589743\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "\n",
    "pbar = tqdm(dataloader_test)\n",
    "for img, target in pbar:\n",
    "    model.reset_states()\n",
    "\n",
    "    img = img.reshape((-1, 28))  # merging time and batch dimensions\n",
    "    out = model.spiking_model(img.to(device))\n",
    "    out = out.reshape((BATCH_SIZE, 28, 10)) # restoring original dimensions\n",
    "    \n",
    "    out = out.sum(1)\n",
    "    predicted = torch.max(out, axis=1)[1]\n",
    "    acc = (predicted == target.to(device)).sum().cpu().numpy() / BATCH_SIZE\n",
    "    accs.append(acc)\n",
    "        \n",
    "print(sum(accs)/len(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value, although not very high, shows as a proof of concept that the persistent state of the spiking network (the membrane potentials) can be exploited as a short-term memory for solving sequential tasks, provided the training procedure takes it into account."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
