[core/auto]
task_type = core/auto/supervised_task
cache_dir = ./cache

# model
[core/model/unilm]
pretrained_name = unilm-base-uncased
num_beams = 5
no_repeat_ngram_size = 0
max_gen_seq_length = 15
repetition_penalty = 1.0

# dataset
[core/dataset]
# data columns: id, num, query, doc, label, score
data_name = fuliucansheng/data_for_test

[core/dataset/train]
preprocess_funcs = ['core/process/unilm_for_faster_generation(query, doc)']

[core/dataset/dev]
preprocess_funcs = ['core/process/unilm_for_tokens(query)', 'core/process/unilm_for_next_tokens(doc)']

[core/dataset/test]
preprocess_funcs = ['core/process/unilm_for_tokens(query)']

# process
[core/process/unilm]
pretrained_name = unilm-base-uncased
max_seq_length = 24
max_gen_seq_length = 15

# task
[core/auto/supervised_task]
model = core/model/unilm_for_generation
optim = core/optim/adam
dataset = core/dataset/auto
loss_fn = core/loss/lm
score_fn = core/score/bleu
monitor_fns = ['core/score/bleu', 'core/score/rouge1', 'core/score/rouge2', 'core/score/rougel']
output_header = query,doc
post_process_fn = partial(core/process/unilm_for_decode)

opt_fp16 = O1
from_ckpt_dir = ${core/auto:cache_dir}
to_ckpt_dir = ${core/auto:cache_dir}
output_path = ${core/auto:cache_dir}/output.txt
train_batch_size = 64
dev_batch_size = 64
test_batch_size = 64
