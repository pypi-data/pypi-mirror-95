import os
import torch
from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union
from collections import OrderedDict
from torch import Tensor, device
import json
import random
from functools import partial
import numpy as np
import torch.nn as nn
import torch.nn.functional as F

from unitorch import add_core_pretrained_dict, register_model
from unitorch.core import core_model_class
from unitorch.utils.hf import hf_cached_path
from unitorch.modules.generation_mixin_v2 import GenerationMixinV2
from transformers.modeling_outputs import Seq2SeqLMOutput
from transformers.configuration_utils import PretrainedConfig
from transformers.file_utils import ModelOutput
from fairseq.modules.multihead_attention import MultiheadAttention
from fairseq.models.transformer import TransformerEncoder, TransformerDecoder
from fairseq.tokenizer import tokenize_line
from fairseq.binarizer import safe_readline
from fairseq.data import data_utils, Dictionary
from fairseq import utils

from unitorch.utils.decorators import (
    generation_model_decorator,
    add_default_section_for_init,
    add_default_section_for_instance_function,
    replace,
)

CONFIG_KEY = "config"
CONFIG_NAME = "config.json"

VOCABS_KEY = "vocab"
VOCABS_NAME = "vocab.txt"

CORE_MASS_PRETRAINED_DICT = {
    "default-mass": {
        "config": "https://huggingface.co/fuliucansheng/mass/resolve/main/mass-base-uncased-config.json",
        "vocab": "https://huggingface.co/fuliucansheng/mass/resolve/main/mass-base-uncased-vocab.txt",
    },
    "mass-base-uncased": {
        "config": "https://huggingface.co/fuliucansheng/mass/resolve/main/mass-base-uncased-config.json",
        "vocab": "https://huggingface.co/fuliucansheng/mass/resolve/main/mass-base-uncased-vocab.txt",
        "weight": "https://huggingface.co/fuliucansheng/mass/resolve/main/mass-base-uncased-pytorch-model.bin",
    },
}

add_core_pretrained_dict(CORE_MASS_PRETRAINED_DICT)


def _reorder_buffer(attn_cache, new_order):
    for k, input_buffer_k in attn_cache.items():
        if input_buffer_k is not None:
            attn_cache[k] = input_buffer_k.index_select(0, new_order)
    return attn_cache


class BertDictionary(Dictionary):
    """A mapping from symbols to consecutive integers"""

    def __init__(
        self,
        pad="<pad>",
        eos="</s>",
        unk="<unk>",
        bos="<s>",
        extra_special_symbols=None,
    ):
        super().__init__(
            pad=pad,
            eos=eos,
            unk=unk,
            bos=bos,
            extra_special_symbols=extra_special_symbols,
        )

    @classmethod
    def load_from_file(cls, filename):
        d = cls()
        d.symbols = []
        d.count = []
        d.indices = {}

        with open(filename, "r", encoding="utf-8", errors="ignore") as input_file:
            for line in input_file:
                k = line.strip()
                d.add_symbol(k)

        d.unk_word = "[UNK]"
        d.pad_word = "[PAD]"
        d.eos_word = "[SEP]"
        d.bos_word = "[CLS]"

        d.bos_index = d.add_symbol("[CLS]")
        d.pad_index = d.add_symbol("[PAD]")
        d.eos_index = d.add_symbol("[SEP]")
        d.unk_index = d.add_symbol("[UNK]")

        d.nspecial = 999
        return d

    def save(self, f):
        """Stores dictionary into a text file"""
        ex_keys, ex_vals = self._get_meta()
        self._save(f, zip(ex_keys + self.symbols, ex_vals + self.count))


@replace(MultiheadAttention)
class MultiheadAttentionV2(MultiheadAttention):
    def __init__(
        self,
        embed_dim,
        num_heads,
        kdim=None,
        vdim=None,
        dropout=0.0,
        bias=True,
        add_bias_kv=False,
        add_zero_attn=False,
        self_attention=False,
        encoder_decoder_attention=False,
        q_noise=0.0,
        qn_block_size=8,
    ):
        super().__init__(
            embed_dim,
            num_heads,
            kdim,
            vdim,
            dropout,
            bias,
            add_bias_kv,
            add_zero_attn,
            self_attention,
            encoder_decoder_attention,
            q_noise,
            qn_block_size,
        )

    def forward(
        self,
        query,
        key: Optional[Tensor],
        value: Optional[Tensor],
        key_padding_mask: Optional[Tensor] = None,
        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,
        need_weights: bool = True,
        static_kv: bool = False,
        attn_mask: Optional[Tensor] = None,
        before_softmax: bool = False,
        need_head_weights: bool = False,
    ) -> Tuple[Tensor, Optional[Tensor]]:
        """Input shape: Time x Batch x Channel
        Args:
            key_padding_mask (ByteTensor, optional): mask to exclude
                keys that are pads, of shape `(batch, src_len)`, where
                padding elements are indicated by 1s.
            need_weights (bool, optional): return the attention weights,
                averaged over heads (default: False).
            attn_mask (ByteTensor, optional): typically used to
                implement causal attention, where the mask prevents the
                attention from looking forward in time (default: None).
            before_softmax (bool, optional): return the raw attention
                weights and values before the attention softmax.
            need_head_weights (bool, optional): return the attention
                weights for each head. Implies *need_weights*. Default:
                return the average attention weights over all heads.
        """
        if need_head_weights:
            need_weights = True

        is_tpu = query.device.type == "xla"

        tgt_len, bsz, embed_dim = query.size()
        assert embed_dim == self.embed_dim
        assert list(query.size()) == [tgt_len, bsz, embed_dim]

        if (
            not self.onnx_trace
            and not is_tpu  # don't use PyTorch version on TPUs
            and incremental_state is None
            and not static_kv
            # A workaround for quantization to work. Otherwise JIT compilation
            # treats bias in linear module as method.
            and not torch.jit.is_scripting()
        ):
            assert key is not None and value is not None
            return F.multi_head_attention_forward(
                query,
                key,
                value,
                self.embed_dim,
                self.num_heads,
                torch.empty([0]),
                torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),
                self.bias_k,
                self.bias_v,
                self.add_zero_attn,
                self.dropout_module.p,
                self.out_proj.weight,
                self.out_proj.bias,
                self.training or self.dropout_module.apply_during_inference,
                key_padding_mask,
                need_weights,
                attn_mask,
                use_separate_proj_weight=True,
                q_proj_weight=self.q_proj.weight,
                k_proj_weight=self.k_proj.weight,
                v_proj_weight=self.v_proj.weight,
            )

        if incremental_state is not None:
            saved_state = self._get_input_buffer(incremental_state)
            if saved_state is not None and "prev_key" in saved_state:
                # previous time steps are cached - no need to recompute
                # key and value if they are static
                if static_kv:
                    assert self.encoder_decoder_attention and not self.self_attention
                    key = value = None
        else:
            saved_state = None

        if self.self_attention:
            q = self.q_proj(query)
            k = self.k_proj(query)
            v = self.v_proj(query)
        elif self.encoder_decoder_attention:
            # encoder-decoder attention
            q = self.q_proj(query)
            if key is None:
                assert value is None
                k = v = None
            else:
                k = self.k_proj(key)
                v = self.v_proj(key)

        else:
            assert key is not None and value is not None
            q = self.q_proj(query)
            k = self.k_proj(key)
            v = self.v_proj(value)
        q *= self.scaling

        if self.bias_k is not None:
            assert self.bias_v is not None
            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = torch.cat(
                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1
                )
            if key_padding_mask is not None:
                key_padding_mask = torch.cat(
                    [
                        key_padding_mask,
                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),
                    ],
                    dim=1,
                )

        q = (
            q.contiguous()
            .view(tgt_len, bsz * self.num_heads, self.head_dim)
            .transpose(0, 1)
        )
        if k is not None:
            kv_bsz = k.size(1)
            k = (
                k.contiguous()
                .view(-1, kv_bsz * self.num_heads, self.head_dim)
                .transpose(0, 1)
            )
        if v is not None:
            kv_bsz = v.size(1)
            v = (
                v.contiguous()
                .view(-1, kv_bsz * self.num_heads, self.head_dim)
                .transpose(0, 1)
            )

        if saved_state is not None:
            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)
            if "prev_key" in saved_state:
                _prev_key = saved_state["prev_key"]
                assert _prev_key is not None
                kv_bsz = _prev_key.size(0)
                prev_key = _prev_key.view(kv_bsz * self.num_heads, -1, self.head_dim)
                if static_kv:
                    k = prev_key
                else:
                    assert k is not None
                    k = torch.cat([prev_key, k], dim=1)
            if "prev_value" in saved_state:
                _prev_value = saved_state["prev_value"]
                assert _prev_value is not None
                kv_bsz == _prev_value.size(0)
                prev_value = _prev_value.view(
                    kv_bsz * self.num_heads, -1, self.head_dim
                )
                if static_kv:
                    v = prev_value
                else:
                    assert v is not None
                    v = torch.cat([prev_value, v], dim=1)
            prev_key_padding_mask: Optional[Tensor] = None
            if "prev_key_padding_mask" in saved_state:
                prev_key_padding_mask = saved_state["prev_key_padding_mask"]
            assert k is not None and v is not None
            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(
                key_padding_mask=key_padding_mask,
                prev_key_padding_mask=prev_key_padding_mask,
                batch_size=kv_bsz,
                src_len=k.size(1),
                static_kv=static_kv,
            )

            saved_state["prev_key"] = k.view(kv_bsz, self.num_heads, -1, self.head_dim)
            saved_state["prev_value"] = v.view(
                kv_bsz, self.num_heads, -1, self.head_dim
            )
            saved_state["prev_key_padding_mask"] = key_padding_mask
            # In this branch incremental_state is never None
            assert incremental_state is not None
            incremental_state = self._set_input_buffer(incremental_state, saved_state)
        assert k is not None
        src_len = k.size(1)

        # This is part of a workaround to get around fork/join parallelism
        # not supporting Optional types.
        if key_padding_mask is not None and key_padding_mask.dim() == 0:
            key_padding_mask = None

        if key_padding_mask is not None:
            assert key_padding_mask.size(0) == kv_bsz
            assert key_padding_mask.size(1) == src_len

        if self.add_zero_attn:
            assert v is not None
            src_len += 1
            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)
            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)
            if attn_mask is not None:
                attn_mask = torch.cat(
                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1
                )
            if key_padding_mask is not None:
                key_padding_mask = torch.cat(
                    [
                        key_padding_mask,
                        torch.zeros(key_padding_mask.size(0), 1).type_as(
                            key_padding_mask
                        ),
                    ],
                    dim=1,
                )

        if self.encoder_decoder_attention == True and bsz != kv_bsz:
            attn_weights = torch.einsum(
                "bxhtd,bhsd->bxhts",
                q.view(kv_bsz, -1, self.num_heads, *q.size()[1:]),
                k.view(kv_bsz, self.num_heads, *k.size()[1:]),
            )
            attn_weights = attn_weights.reshape(-1, *attn_weights.size()[-2:])
        else:
            attn_weights = torch.bmm(q, k.transpose(1, 2))

        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)

        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]

        if attn_mask is not None:
            attn_mask = attn_mask.unsqueeze(0)
            if self.onnx_trace:
                attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)
            attn_weights += attn_mask

        if key_padding_mask is not None:
            # don't attend to padding symbols
            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            attn_weights = attn_weights.view(
                kv_bsz, -1, self.num_heads, tgt_len, src_len
            )

            if not is_tpu:
                attn_weights = attn_weights.masked_fill(
                    key_padding_mask.unsqueeze(1)
                    .unsqueeze(2)
                    .unsqueeze(3)
                    .to(torch.bool),
                    float("-inf"),
                )
            else:
                attn_weights = attn_weights.transpose(0, 2)
                attn_weights = attn_weights.masked_fill(key_padding_mask, float("-inf"))
                attn_weights = attn_weights.transpose(0, 2)
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)

        if before_softmax:
            return attn_weights, v

        attn_weights_float = utils.softmax(
            attn_weights, dim=-1, onnx_trace=self.onnx_trace
        )
        attn_weights = attn_weights_float.type_as(attn_weights)
        attn_probs = self.dropout_module(attn_weights)

        assert v is not None
        if self.encoder_decoder_attention == True and bsz != kv_bsz:
            attn = torch.einsum(
                "bxhts,bhsd->bxhtd",
                attn_probs.view(kv_bsz, -1, self.num_heads, *attn_probs.size()[1:]),
                v.view(kv_bsz, self.num_heads, *v.size()[1:]),
            )
            attn = attn.reshape(-1, *attn.size()[-2:])
        else:
            attn = torch.bmm(attn_probs, v)
        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]
        if self.onnx_trace and attn.size(1) == 1:
            # when ONNX tracing a single decoder step (sequence length == 1)
            # the transpose is a no-op copy before view, thus unnecessary
            attn = attn.contiguous().view(tgt_len, bsz, embed_dim)
        else:
            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
        attn = self.out_proj(attn)
        attn_weights: Optional[Tensor] = None
        if need_weights:
            attn_weights = attn_weights_float.view(
                bsz, self.num_heads, tgt_len, src_len
            ).transpose(1, 0)
            if not need_head_weights:
                # average attention weights over heads
                attn_weights = attn_weights.mean(dim=0)

        return attn, attn_weights


class MassConfig(PretrainedConfig):
    def __init__(self, params: dict):
        super().__init__(**params)
        self.adaptive_softmax_cutoff = None
        self.max_source_positions = params.get("max_source_positions", 512)
        self.max_target_positions = params.get("max_target_positions", 512)
        self.pad_token_id = params.get("pad_token_id", 0)


class Mass(core_model_class):
    def __init__(self, config_path, **kwargs):
        super().__init__()
        self.config_path = config_path
        self.config = MassConfig(json.load(open(self.config_path, "r")))

    @classmethod
    def init_from_core_configure(cls, cfg, **kwargs):
        pass

    def save_checkpoint(self, ckpt_dir="./cache", **kwargs):
        if not os.path.exists(ckpt_dir):
            os.mkdir(ckpt_dir)
        config_path = os.path.join(ckpt_dir, CONFIG_NAME)
        with open(config_path, "w") as f:
            f.write(open(self.config_path, "r").read())

        super().save_checkpoint(ckpt_dir=ckpt_dir, **kwargs)

    def from_checkpoint(self, ckpt_dir="./cache", **kwargs):
        super().from_checkpoint(ckpt_dir=ckpt_dir, **kwargs)


@register_model("core/model/mass_for_generation", generation_model_decorator)
class MassForGeneration(Mass, GenerationMixinV2):
    def __init__(self, config_path, vocab_path):
        super().__init__(config_path)

        self.dictionary = BertDictionary.load_from_file(vocab_path)
        self.shared = nn.Embedding(
            self.config.vocab_size, self.config.encoder_embed_dim, 0
        )

        self.encoder = TransformerEncoder(
            self.config, dictionary=self.dictionary, embed_tokens=self.shared
        )
        self.decoder = TransformerDecoder(
            self.config,
            dictionary=self.dictionary,
            embed_tokens=self.shared,
        )
        self.init_weights()

    def get_input_embeddings(self):
        return self.shared

    def set_input_embeddings(self, value):
        self.shared = value
        self.encoder.embed_tokens = self.shared
        self.decoder.embed_tokens = self.shared

    def get_encoder(self):
        return self.encoder

    def get_decoder(self):
        return self.decoder

    @property
    def device(self) -> device:
        """
        :obj:`torch.device`: The device on which the module is (assuming that all the module parameters are on the same
        device).
        """
        try:
            return next(self.parameters()).device
        except StopIteration:
            # For nn.DataParallel compatibility in PyTorch 1.5

            def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:
                tuples = [
                    (k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)
                ]
                return tuples

            gen = self._named_members(get_members_fn=find_tensor_attributes)
            first_tuple = next(gen)
            return first_tuple[1].device

    @classmethod
    @add_default_section_for_init("core/model/mass")
    def init_from_core_configure(cls, cfg, **kwargs):
        pretrained_name = cfg.getdefault(
            "core/model/mass", "pretrained_name", "default-mass"
        )
        config_name_or_path = cfg.getdefault(
            "core/model/mass", "config_name_or_path", pretrained_name
        )
        config_path = (
            CORE_MASS_PRETRAINED_DICT[config_name_or_path][CONFIG_KEY]
            if config_name_or_path in CORE_MASS_PRETRAINED_DICT
            else config_name_or_path
        )

        config_path = hf_cached_path(config_path)

        vocab_name_or_path = cfg.getdefault(
            "core/model/mass", "vocab_name_or_path", pretrained_name
        )
        vocab_path = (
            CORE_MASS_PRETRAINED_DICT[vocab_name_or_path][VOCABS_KEY]
            if vocab_name_or_path in CORE_MASS_PRETRAINED_DICT
            else vocab_name_or_path
        )

        vocab_path = hf_cached_path(vocab_path)

        inst = cls(config_path, vocab_path)
        if pretrained_name is not None:
            inst.from_pretrained(pretrained_name)

        return inst

    def _prepare_encoder_decoder_kwargs_for_generation(
        self, input_ids: torch.LongTensor, model_kwargs
    ) -> Dict[str, Any]:
        # retrieve encoder hidden states
        encoder = self.get_encoder()
        encoder_kwargs = {
            argument: value
            for argument, value in model_kwargs.items()
            if not argument.startswith("decoder_")
        }
        encoder_outputs = encoder(input_ids, src_lengths=input_ids.ne(0).sum(-1))
        model_kwargs["encoder_outputs"] = ModelOutput(
            dict(
                encoder_out=encoder_outputs.encoder_out,
                last_hidden_state=encoder_outputs.encoder_out.transpose(0, 1),
                encoder_padding_mask=encoder_outputs.encoder_padding_mask,
            )
        )
        return model_kwargs

    def prepare_inputs_for_generation(
        self,
        decoder_input_ids,
        past=None,
        attention_mask=None,
        head_mask=None,
        use_cache=None,
        encoder_outputs=None,
        **kwargs,
    ):
        if past is not None:
            decoder_input_ids = decoder_input_ids[:, -1:]

        if hasattr(encoder_outputs, "last_hidden_state"):
            delattr(encoder_outputs, "last_hidden_state")

        return {
            "decoder_input_ids": decoder_input_ids,
            "attention_mask": attention_mask,
            "encoder_outputs": encoder_outputs,
            "incremental_state": past,
        }

    @staticmethod
    def _reorder_cache(past, beam_idx):
        reordered_past = OrderedDict()
        for i, (key, attn_cache) in enumerate(past.items()):
            if i % 2 == 0:
                reordered_past[key] = _reorder_buffer(attn_cache, beam_idx)
            else:
                reordered_past[key] = attn_cache
        return reordered_past

    @staticmethod
    def _reorder_cache_v2(past, batch_idx, beam_idx):
        reordered_past = OrderedDict()
        for i, (key, attn_cache) in enumerate(past.items()):
            if i % 2 == 0:
                reordered_past[key] = _reorder_buffer(attn_cache, beam_idx)
            else:
                reordered_past[key] = _reorder_buffer(attn_cache, batch_idx)
        return reordered_past

    def forward(
        self,
        tokens_ids_a=None,
        tokens_ids_b=None,
        decoder_input_ids=None,
        encoder_outputs=None,
        incremental_state=None,
        decoder_length=None,
        attention_mask=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        if self.training:
            encoder_outputs = self.encoder(
                tokens_ids_a, src_lengths=tokens_ids_a.ne(0).sum(-1)
            )
            decoder_outputs = self.decoder(tokens_ids_b, encoder_out=encoder_outputs)
            return decoder_outputs[0]
        if incremental_state is None:
            incremental_state = OrderedDict()

        decoder_outputs = self.decoder(
            decoder_input_ids,
            encoder_out=encoder_outputs,
            incremental_state=incremental_state,
        )
        return Seq2SeqLMOutput(
            logits=decoder_outputs[0], past_key_values=incremental_state
        )

    @add_default_section_for_instance_function("core/model/mass")
    def generate(
        self,
        tokens_ids,
        num_beams=5,
        decoder_start_token_id=101,
        decoder_end_token_id=102,
        num_return_sequences=1,
        min_gen_seq_length=0,
        max_gen_seq_length=48,
        repetition_penalty=1.0,
        no_repeat_ngram_size=0,
        early_stopping=True,
        length_penalty=1.0,
        num_beam_groups=1,
        diversity_penalty=0.0,
        diverse_rate=0.0,
    ):
        self.num_beams = num_beams
        batch_hyp = super().generate(
            tokens_ids,
            max_length=max_gen_seq_length,
            min_length=min_gen_seq_length,
            num_beams=num_beams,
            do_sample=False,
            decoder_start_token_id=decoder_start_token_id,
            no_repeat_ngram_size=no_repeat_ngram_size,
            early_stopping=early_stopping,
            length_penalty=length_penalty,
            repetition_penalty=repetition_penalty,
            num_return_sequences=num_return_sequences,
            bos_token_id=decoder_start_token_id,
            eos_token_id=decoder_end_token_id,
            num_beam_groups=num_beam_groups,
            diversity_penalty=diversity_penalty,
        )
        batch_hyp = batch_hyp.reshape(-1, num_return_sequences, batch_hyp.size(-1))
        if batch_hyp.size(-1) >= max_gen_seq_length:
            return batch_hyp[:, :, :max_gen_seq_length]
        batch_ret = torch.zeros(
            batch_hyp.size(0), num_return_sequences, max_gen_seq_length
        )
        batch_ret[:, :, : batch_hyp.size(-1)].copy_(batch_hyp)
        return batch_ret.long()
